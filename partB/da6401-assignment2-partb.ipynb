{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11337430,"sourceType":"datasetVersion","datasetId":7092365}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#--------------------\n# Necessary Libraries\n#--------------------\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.utils.data import DataLoader, SubsetRandomSampler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#----------------------\n# Log in to W&B account\n#----------------------\nimport wandb\nwandb.login(key='150002a34bcf7d04848ccaff65ab76ca5cc3f11b')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-------------------------\n# Inaturalist-dataset path\n#-------------------------\ndata_dir = \"/kaggle/input/inaturalist-dataset/nature_12K/inaturalist_12K\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#---------------------------\n# ResNet50 Finetune Function\n#---------------------------\ndef load_resnet50_finetune_model(\n    mode=\"head\",                 # options: \"head\", \"partial\", \"full\"\n    num_classes=10,\n    dropout_rate=0.0,\n    unfreeze_from=\"layer3\"       # if mode == \"partial\"\n):\n    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n    # Replace the final fully connected layer\n    in_features = model.fc.in_features\n    model.fc = nn.Sequential(\n        nn.Dropout(dropout_rate),\n        nn.Linear(in_features, num_classes)\n    )\n\n    if mode == \"head\":\n        # Freeze all layers except the final classification head\n        for name, param in model.named_parameters():\n            if not name.startswith(\"fc\"):\n                param.requires_grad = False\n\n    elif mode == \"partial\":\n        # Partially unfreeze the model, training layers from 'unfreeze_from' onwards\n        freeze = True\n        for name, module in model.named_children():\n            if name == unfreeze_from:\n                freeze = False\n            for param in module.parameters():\n                param.requires_grad = not freeze\n\n    elif mode == \"full\":\n        # Unfreeze the entire model, training all layers\n        for param in model.parameters():\n            param.requires_grad = True\n\n    else:\n        raise ValueError(\"Mode must be one of: 'head', 'partial', or 'full'\")\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-------------------------------\n# Data Loading and Preprocessing\n#-------------------------------\ndef load_and_preprocess_data(data_dir, batch_size, validation_split=0.2):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),                   # Resize images\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])   # Standardize the pixel values\n    ])\n\n    train_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform)\n    test_dataset = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=transform)\n\n    # Training and validation splits\n    dataset_size = len(train_dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n\n    return train_loader, val_loader, test_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-------------------\n# Training the model\n#-------------------\ndef train(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for images, labels in train_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Train loss and Accuracy\n            train_loss += loss.item() * labels.size(0)  # Accumulate loss\n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            \n        avg_train_loss = train_loss / total\n        train_accuracy = correct / total\n\n        # Validation\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        val_loss = 0.0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images = images.to(device)\n                labels = labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * labels.size(0)\n                _, predicted = torch.max(outputs.data, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n                \n        avg_val_loss = val_loss / val_total\n        val_accuracy = val_correct / val_total\n\n        # Log in wandb\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": avg_train_loss,\n            \"train_accuracy\": train_accuracy,\n            \"val_loss\": avg_val_loss,\n            \"val_accuracy\": val_accuracy\n        })\n        print(f\"Epoch [{epoch+1}/{epochs}], Validation Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#--------------------------\n# Wandb Sweep Configuration\n#--------------------------\nsweep_config = {\n    \"method\": \"bayes\",  # Bayesian optimization for efficiency\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'epochs' : {'values':[5,10]},\n        #'mode': {'values': ['head', 'partial', 'full']},\n        'dropout': {'values': [0.0, 0.2, 0.3]},\n        #'unfreeze_from': {'values': ['layer2', 'layer3', 'layer4']},\n        'batch_size': {'values': [32, 64]},\n        'learning_rate': {'values': [1e-3, 1e-4]}\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#---------------------\n# Wandb Sweep Function\n#---------------------\ndef wandb_sweep():\n    \n    wandb.init(project=\"iNaturalist-ResNet50\")\n    # Access sweep configuration from wandb\n    config = wandb.config\n\n    # Run name\n    run_name = f\"ep-{config.epochs}_dro-{config.dropout}_bs-{config.batch_size}_lr-{config.learning_rate}\"\n    wandb.run.name = run_name\n\n    # Data Loading\n    data_dir = \"/kaggle/input/inaturalist-dataset/nature_12K/inaturalist_12K\"\n    train_loader, val_loader, test_dataset = load_and_preprocess_data(data_dir, config.batch_size)\n\n    model = load_resnet50_finetune_model(\n        mode='head',   #config.mode,\n        dropout_rate=config.dropout,\n        #unfreeze_from=config.unfreeze_from,\n        num_classes=10\n    )\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n\n    # Training\n    train(model, train_loader, val_loader, optimizer, criterion, epochs=config.epochs, device=device)\n\n    # Evaluate on test set\n    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        accuracy = correct / total\n        print(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    sweep_id = wandb.sweep(sweep_config, project=\"iNaturalist-ResNet50\")\n    wandb.agent(sweep_id, wandb_sweep)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}